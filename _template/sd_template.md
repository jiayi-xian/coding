### Recording

#### harmful content detection

##### dataset construction

Before we do model training, we need to construct the data set. The data set comprise model inputs, which are features and outputs, which are labels to construct inputs. We process the post of the pins of lye in benches and compute fuels features as described earlier. Those features can be stored in a feature store for future training or for training of other models. In order to create the labels for each sample, we have several options. I we can ask the human annotator to do a hand labeling. We could also use the let's call it natural labeling, which means we collect those samples that have been reported from users or flagged by users. Those are our positive samples. And also we might have for data pipeline, we could do a semi supervised and like user, teachers, students, they still network to probably automatically do labeling for the samples.

So in this setting, for each samples, we expect that it has a vector zero one vectors for each of the coordinates of the vector. It indicates that if it has a for example, illegal goods, or if it has a violence content, or if it has self harm, and so on.

##### loss function

Now, we knew we are going to choose the loss function since we are going to build a multitask neural network, we're going to choose the and we do a classification for each of the subclass, which is also a binary classification. We each task is assign a loss function based on its ml category. In our case, each task is frame as a binary classification. We adopt ABC binary cross entropy. For each task, the overall loss is computed by combining a task specific losses. We could combine the waited task specific losses.

##### Training (challenge)

A common Challenge in training multimodal systems is over fitting. For example, when the learning speed varies across different modality, one modality can dominate the learning process. Two techniques to address these issues are gradient bending and focal lost.

##### Model architecture

When regarding to the model architecture, I would like to have a multitask classifier. This basically a more DNN that have a share button and separate classification head. For example, now let's say we have an input that we have already put some feature engineering on it. And then it has a let's say, we started from a representation of the input, which will be a the vector is basically a fused features. Then we put this, go through our share layer, which would be probably two linear layers. Then we have an intermediate output. This in the media output, we push it through several like a different classification hat. Each of the classification hat will be like one or two linear layers. On the output. It will go through the sigma function to predict a probability for each of the supplies, which means what the output of.
For example, now, let's say we have a violence class class, then the output of this classification had would be a probability showing how likely the inputs will have a violence contents. Similar the similar to it for the other classification hat. This model in this model, we use a share layers. Usually we call it share bottom. We hope this will learn. And this share button usually functions as like a combination of feature extractor, feature transformation, and feature interactions, kind of that.
So basically, we hopefully the the input feature after got part after propagate forward of these share buttons, it would be ordered, lay all the features, even if sorry, image features or text, textual features, or numerical or category of feature, they would be mapped or transformed to our unified space, unified lantern space.
And from there, the feature could be interacting with each other more, meaningfully or efficiently for the classification. Had we expected to learn? The difference can combination of the features that can contribute to this class. That's the whole picture of the multitask classifier, or the benefit of it as first, when it compared to our like a simple model that just output one probability saying that is safe or not, it has more flexibility and more expandability. With this. We could show why we need to take down the poles or the pins of the other.
Also, sometimes I different difference of class may have different features or patterns that so we probably separate them apart better than mix them UPS. Since some signals might like cancellation with the other, some signals might. It's possible that for some of class, for like violence, it has its own signals. And for the nude nudity, they have its own patterns. We separate them instead of miss them together, because sometimes the mixture may like the store, each other, like distress or distort each other a little bit.
In so also there, in addition, there are the share bottom parts. We in this case, we don't need to separate the we don't need to have a set, have several separate models. For each of the subclass. In this case, we also like improve the utilization of the data, which means each day data samples contribute to somehow contribute to the each of the prediction of the class. Since usually, our case, we probably will face the data imbalance issue us due to the the reality, the rarity of the due to the rarity of the positive sample that reported of flag when compared to a huge amount of normal pins. That's a whole picture of the.

##### model training 2

For the model training, a common Challenge in training multi model. System is over fitting, which means the learning speeds varies across different modalities. One modality might dominate the learning process. Two techniques to address these issues are gradient blending and focal lost. Also, we could suggest to use their to utilize the clip model, which means we use the image encoder and tax encoder from the clip model, because they during the training of the model, they were trained to map the to map the features from to map the image feature and the text feature to our would be the same lantern space and those feature a lie, while the output of the encoder alike quite well for the encoders.
For the clip encoders, I think if we put them in this model, it will improve the model performance. Also. There are the convergence. Now, let's talk about the evaluation. Usually we have offline matrix and online matrix of line matches. Since we have a classification models, we could use the auc or PR curve, PR au like auc of the PR curve and auc of the roc the PR curve shows the tradeoff between the precision and recall of the model.

##### evaluation

This is important for us since we usually a a high prauc indicates a more accurate model for the online matrix. Also, we have like precision record, f one score, f beta score, and accuracy, and so on. For the for the offline matrix, we also have this magic. We could also like consider each of these magic for each of the subclass. Since we this could show, this would have insights on which our model strong at which our model shot at. For the online metric we have for the online metrics. We have prevalence means prevalence, measures the ratio of harm, harmful pulse. That's to give away from our models and skip away from our models. And we could have also have the harmful impression. This is more accurately. It has more accurate measure to measurement of how worse, how bad in impressions, how bad impacts due to the exposure of the un unsafe, how the harmful is impression, capture the Information, like how bad the exposures, the scheme away, unsafe content has been made.
We also have valid appeals or proactive rates. They are trade off the valley appeals measures is a percentage of posts that have been predicted as predicted as unsafe but revoked manually divided by the number all the polls have been in predicted as unsafe and proactive.
This was more like a a like a false positive rates and proactive rates as the number of the number of a harmful post detected by the system are divided by all the post, all the divided by the number of harmful posts detected by the system and reported by the users.
This is our predicted is our the samples are predicting, the samples predicted as positive divided by the ground truth or positive. This is more like precision. This is more like recall, this more like a recall, and appeals raise more like like a precision. The appeals are more like a false positive rate. And proactive is more have some like a sense of like a recall. We could also have user reports for each of the harmful class.
