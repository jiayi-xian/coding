{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 0 0 0 2 1 0 2 2 0 0 1 2 1 2 0 1 2 1 0]\n","[0 2 0 1 0 0 2 2 1 0 1 2 0 2 1 2 2 2 1 2]\n"]}],"source":["import numpy as np\n","\n","y = np.array(range(20)) # TODO: np.random.choice ??\n","N =20\n","K = 3\n","y = np.random.multinomial(N, [1/K]*K, size=1) # 用来构建C比较合适 C[i]:i类sample的个数\n","y = np.random.choice(K, N) # range, size/shape, probabilities(optional)\n","y_pred =np.random.choice(K, N)\n","\n","print(y)\n","print(y_pred)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.33333333 0.2       ] [0.25       0.16666667] [0.14285714 0.09090909]\n"]}],"source":["def metrics_compute(Y, Y_pred):\n","    \n","    \"\"\"\n","    Y : (N, K) \n","    Y_pred: (N, K)\n","    \n","    \"\"\"\n","    #y_pred = np.argmax(Y_pred, axis = 1)\n","    #y = np.argmax(Y, axis = 1)\n","    C_size = np.array([sum(y==i) for i in range(np.min(y), np.max(y))])\n","    \n","    TP, FP, TN, FN = [], [], [], []\n","    precision, recall, f1 = [], [], []\n","    \n","    # get tp, fp, tn, fn for each class\n","    for i in range(len(C_size)):\n","        tp = np.sum(y_pred[y==i]==i)\n","        fp = np.sum(y_pred[y!=i]==i)\n","        tn = np.sum(y_pred[y!=i]!=i)\n","        fn = np.sum(y_pred[y==i]!=i)\n","        \n","        TP.append(tp)\n","        FP.append(fp)\n","        TN.append(tn)\n","        FN.append(fn)\n","    \n","    TP, FP, TN, FN = np.array(TP), np.array(FP), np.array(TN), np.array(FN)\n","    Precision = TP/(TP+FP)\n","    Recall = TP/C_size\n","    F1 = Precision*Recall/(Precision+Recall)\n","    print(Precision, Recall, F1)\n","\n","metrics_compute(y, y_pred)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[8, 6, 6]\n","[6, 5, 9]\n","[2, 1, 2] [4, 4, 7]\n"]}],"source":["C = [y[y==i] for i in range(y.min(), y.max()+1)]\n","\"\"\"\n","[array([0, 0, 0, 0, 0, 0, 0, 0]),\n"," array([1, 1, 1, 1, 1, 1]),\n"," array([2, 2, 2, 2, 2, 2])]\n","\"\"\"\n","C_size = [len(y[y==i]) for i in range(y.min(), y.max()+1)]\n","C_pred_size = [len(y_pred[y_pred==i]) for i in range(y.min(), y.max()+1)]\n","print(C_size)\n","print(C_pred_size)\n","TP, FP = [], []\n","for i in range(len(C_size)):\n","    TP.append(np.sum(y_pred[y==i]==i))\n","    FP.append(np.sum(y_pred[y!=i]==i))\n","\n","print(TP, FP)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","# $y = \\frac{1}{1 + e^{WX+B}}$\n","\n","class LogisticRegression:\n","    \n","    def __init__(self, X=None, Y=None,penalty = \"L2\", _intercept=True) -> None:\n","        self.W = None\n","        self.intercept = _intercept\n","        self.is_fit = False\n","        self.X = X\n","        self.Y = Y\n","        self.gamma = 0 # scaler of penalty\n","        self.penalty = penalty\n","\n","    @staticmethod\n","    def sigmoid(x):\n","        # x: (N,)\n","        return 1/(1+ np.exp(-x)) # broadcast\n","\n","    def predict(self, X): # input: X\n","        \"\"\"\n","        X: (N,M)\n","        \"\"\"\n","        # X -> (N, M+1)\n","\n","        if not self.W:\n","            if self.intercept:\n","                X = np.c_[np.ones(X.shape[0]), X] # 在最左端加了一列，全是1， 这样W是(M+1,) 第一列是B\n","                self.W = np.random.rand(X.shape[1]+1)\n","            else:\n","                self.W = np.random.rand(X.shape[1])\n","        \n","        if not self.is_fit and (self.X and self.Y):\n","            self.fit(self.X, self.Y, iter_max=100, epsilon=0.02)\n","        \n","        return LogisticRegression.sigmoid(X*self.W) # -> X@ self.W\n","\n","    def fit(self, X, Y, iter_max, epsilon):\n","        \"\"\"\n","        X: (N,M)\n","        Y: (N,)\n","        W: (M,)\n","        \"\"\"\n","        self.is_fit = True\n","        # get y_predict\n","        y_pred = self.predict(X)\n","\n","        # get loss function\n","        order = 1 if self.penalty==\"L1\" else 2\n","        # TODO review penalty\n","        penalty = 1/2 * np.sqrt(np.linalg.norm(self.W, ord=order))\n","\n","        # get gradient\n","\n","        # update W\n","\n","    def logloss(self, X, Y_pred, Y_ground):\n","        \"\"\"\n","        X: (N,M)\n","        Y: (N,)\n","        \"\"\"\n","\n","        N = X.shape[0]\n","        loss = -(np.log(Y_pred[Y_ground==1]).sum(axis=0) + np.log(1-Y_pred[Y_ground==0]).sum(axis=0))\n","        loss = -(np.sum(np.log(Y_pred[Y_ground==1]),axis=0) + np.sum(np.log(1-Y_pred[Y_ground==0]),axis=0))\n","        return loss / N\n","    \n","    def _penalty_gradient(self, X, Y_pred, Y_ground):\n","        \"\"\"\n","        if penalty == \"L1\":\n","            return self.gamma\n","        if penalty == \"L2\":\n","            return np.sum(np.abs(self.W))\n","        \"\"\"\n","        # TODO review : datetime\n","        d_penalty = self.gamma * self.W if self.penalty == \"l2\" else self.gamma * np.sign(self.W)\n","        return d_penalty\n","\n","    def _gradient_logloss(self, X, Y_pred, Y_ground):\n","        # gradient of logloss NN->logit->softmax->cross entropy (logloss)\n","        # (f(x_i) - y_i) \\cdot x_i\n","        # Y_pred, Y_ground: (N, K) 不是 K logistic regression是二分类问题 (N,)\n","        # X: (N, M+1)\n","        # Y: (N,)\n","\n","        loss_gradient = X.T@(Y_pred - Y_ground) # TODO review: datetime\n","        return loss_gradient\n","    \n","    def gradient(self, X, Y, Y_pred, Y_ground):\n","\n","        N = len(X)\n","        return 1/N*(self._penalty_gradient(X, Y, Y_pred, Y_ground) + self._gradient_logloss(X, Y, Y_pred, Y_ground))\n","\n","\n","\n","def gradient_Jacobian(X, f):\n","    \"\"\"\n","    X: (M，) -> (M+1,)\n","    \"\"\"\n","    # step 1 function: aver f_loss(X), current point: W\n","\n","    # step 2 (f_loss((W + deltaW, X) - f_loss((W + deltaW, X)) / 2*deltaW 方向导数\n","\n","\n","    for idx, val in enumerate(X):\n"]},{"cell_type":"markdown","metadata":{},"source":["# binary classification\n","\n","\n","def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n","\n","    # preparation(optional)\n","    # initialize parameters\n","    ---iteration starts -----\n","    # iteration condition: iter<max_iter, l_prev - loss < tol, ...\n","    # step 1 Get y_pred\n","    # step 2 get objective function\n","        # step 2.1 get loss -> depends on loss function: log loss, MSE, loss function of VAE, ...\n","        # stpe 2.2 (optional) get penalty -> depends on penalty: l1, l2, ...\n","    # step 3 get gradient of objective function\n","        # step 2.1 get gradient of loss function\n","        # step 2.2 get gradient of penalty\n","    # step 4 update parameters or converge to return\n","        W = W - lr * deltaW\n","    ----iteration ends ----"]}],"metadata":{"kernelspec":{"display_name":"py39","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":2}
